{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "Given a corpus, the content can be divided in **topics**, which are defined as distributions over words, where their order is not important. A document can have multiple topics.\n",
    "\n",
    "\n",
    "\n",
    "## Latent Dirichlet Allocation\n",
    "\n",
    "Posterior inference of unknown random variables (**latent**) from observed variables (topics and words). For each topic computes words **allocation** and for each document computes topic distribution. We assume that words and topics follow a **Dirichlet** distribution, a multivariate variant of the beta distribution.\n",
    "\n",
    "TO perform inference, we associate documents and topics to as few topics and words as possible, which often results in having words that occur together for multiple topics. We represent those with vector of values varying from 0 to 1 that are \"how much the document or the topic exhibits i-th topic/word\".\n",
    "\n",
    "This applies to new documents by assuming the computed distributions are right and inferring posteriors of topic assignment.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
