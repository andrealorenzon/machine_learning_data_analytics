{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-based methods\n",
    "\n",
    "7 november 2018, slides 49-69\n",
    "\n",
    "A  **decision tree** is a supervised machine learning technique which could be used to predict or infer results based on a series of subsequent steps, which could be inferred from data using a recursive **dividi et impera** approach on both tree branches. In practice, you can only proceed from the root towards the leaves.\n",
    "\n",
    "A **branch node** is a node in which we have many possible outcomes. A **decision node** is a node in which a decision about the predicted value is taken.\n",
    "\n",
    "To build a decision tree, we'll need a matrix of features `X` and a vector of outcomes `y`. \n",
    "\n",
    "The **termination criterion** of the recursive function is based only on the labels, choosing as predicted y the most common class in y. It produces  a decision node, which is the simplest tree possible.\n",
    "\n",
    "We select a **branch** characterized by `i` $\\in [1, p]$ that specifies the index of the feature and `t` which is the selection criterion. We use those value to split the remaining tree in two and append the result to the one of this function.\n",
    "\n",
    "This procedure is called **recursive binary splitting**.\n",
    "\n",
    "The **best branch** function is simply minimizing the error. For example, if by assuming all the values on one side are of the chosen value we made 2 errors over 10 total values, we have that \n",
    "\n",
    "$$E(Y) = \\frac{|\\{y \\in Y : y \\neq \\hat{y}\\}}{|Y|} = 0.2$$\n",
    "\n",
    "$arg min_{i,t} E(y|x_i \\geq t) + E(y|x_i < t)$ minimize the classification error in the previous function by considering many split and choosing the one which have the lowest error on both sides of the split.\n",
    "\n",
    "Best branch is a **greedy algorithm** since it seeks to minimize the errors in the fastest possible way.\n",
    "\n",
    "The way of computing the pair $(i^{*}, t^{*})$ could be different, especially in computational time. It is possible that multiple pairs minimize the error, in which case the splitting pair could be selected in different ways (randomly, in order, etc.) depending on implementation.\n",
    "\n",
    "The **termination criterion** is based on a function which returns true if the split y contains only one class, or if the absolute value of y is smaller than a user-provided parameter $k_{min}$. Another possible implementation could take in account another parameter, $d_{max}$, which specified the maximal depth for the tree.\n",
    "\n",
    "From scientific literature, it has been shown that **classificiation error** $E()$, which is the proportion of labels which are different from the most popular on total labels, is not sufficiently sensitive for tree growing. Other better options, which have the same semantics (high level bad, low level good) are:\n",
    "\n",
    "* **Gini index**\n",
    "\n",
    "* **Cross-entropy**\n",
    "\n",
    "Those indexes are represented by smoother functions, making them easier to be optimized. For a multiclass problem, the essence is the same.\n",
    "\n",
    "In general, **classification problems have categorical outcomes**, defined as variables with discrete domains, without ordering.\n",
    "\n",
    "Random trees can mix categorical and numerical variables. In the case of numerical variables, the i is considered as $x_i = c$, while for categorical ones is $x_i \\in C' \\subset C$ with $c$ being a class. $C'$ is a specific subset (class) of a categorical variable (ex. blonde for variable `hair`)\n",
    "\n",
    "A problem that could occur is that tree complexity is too high for the problem it is trying to model. This makes it less understandable. and it is a behavior which is likely driven by noise in carousel data. This is called **overfitting** since the system is fitting the data and its errors, while we wanted insights on the underlying context. Overfitting models have **high variance** since the model can vary a lot if learning data varies.\n",
    "\n",
    "Managing the parameter $k_{min}$ is very important, since it allows the system to prevent overfitting by keeping it high, or to correctly model systems with great complexity by setting it high. Other ways of fighting overfitting for trees are:\n",
    "* limiting their depth with the $d_{max}$ parameter\n",
    "* stopping the node creation if splits doesn't decrease the impurity\n",
    "* use **pruning**\n",
    "\n",
    "**Pruning** is a practice in which, starting from a full tree, we build a sequence of subtrees which have a decreasing level of complexity. At the end, we choose the model having the lowest classification error with k-folds cross validation. In practice, we cut the lower parts of the graph and choose the best one.\n",
    "\n",
    "**How to split data for cross-validation?** \n",
    "Most of the time, we can split it randomly to preserve casuality. If we can split leaving out a specific class each time, though, we can also measure the behavior of a tree in this context.\n",
    "\n",
    "We should test different values for $k_m$ for all the splits, in order to spot the best possible value for the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
