{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximal Margin Classifier and Soft Margin Classifier\n",
    "\n",
    "We can use a line in order to separate data in a binary classification problem. This line could be a curve, or even a curved plane when the feature space is p-dimensional. We call it **separating hyperplane**.\n",
    "\n",
    "For predicting a new value, we simply replace the values of data in the equation and see if the result is above or below zero. **The larger the distance of the result value from 0** (which represent a point exactly on the line) **, the stronger the confidence**.\n",
    "\n",
    "Since there could be infinite lines to divide the two classes, we look for the **line which maximises the distance with the closest point**. This line is called <span style=\"color:blue\">**Maximal Margin Classifier**</span>. The points used to establish this lines are called <span style=\"color:blue\">**Support Vectors**</span>, since they support the behavior of the line and have vectorial form in p-dimensional spaces. The distance between the classifier and the support vectors is called **Margin**.\n",
    "\n",
    "This can be formalized as follows:\n",
    "\n",
    "$max_{\\beta_0,\\dots,\\beta_p} M$ is the **objective**\n",
    "\n",
    "$\\Sigma_{j = 1}^p \\beta_j^2 = 1$ is the **constraint**. This forces the final value to be the Carthesian distance between $x_i$ and the hyperplane.\n",
    "\n",
    "If we define the class as $y \\in \\{-1;1\\}$, we can use a single inequality to express both cases (since the sign will be switched when $y = -1$)\n",
    "\n",
    "$$\\forall i \\in \\{1,\\dots,n\\}, y_i(\\beta_0 + \\beta_1x_{i,1} + \\dots + \\beta_px_{i,p} \\geq M$$\n",
    "\n",
    "**Note: this is valid only for binary classification problems, since the sign trick won't work for multiple classes!**\n",
    "\n",
    "Given that equation, we can now define support vectors as the points for which the following equation is verified:\n",
    "\n",
    "$$\\forall i \\in \\{1,\\dots,n\\}, y_i(\\beta_0 + \\beta_1x_{i,1} + \\dots + \\beta_px_{i,p} = M$$\n",
    "\n",
    "Given this assumption, we have now two problems:\n",
    "\n",
    "* If data is not perfectly separable, we cannot use this technique\n",
    "\n",
    "* The model has **high variance** since adding/removing a support vector can greatly vary the hyperplane shape.\n",
    "\n",
    "In order to avoid those problems, we can use a **soft margin**, being tolerant for misclassified values and values that fall between the margin. This new technique is called **soft margin classifier** or <span style=\"color:blue\">**Support Vector Classifier**</span>.\n",
    "\n",
    "In order to take this new options in account, we add an **error coefficient** $\\epsilon$ for each of n observation (note that, instead, we only have p $\\beta$ ). The equation is changes as:\n",
    "\n",
    "$$\\forall i \\in \\{1,\\dots,n\\}, y_i(\\beta_0 + \\beta_1x_{i,1} + \\dots + \\beta_px_{i,p} \\geq M(1 - \\epsilon_i)$$\n",
    "\n",
    "with the additional condition $\\forall i \\in \\{1,\\dots,n\\}, \\epsilon_i \\geq 0$ and $\\Sigma^n_{j = 1}\\epsilon_j = C$, where $C$ is the **toleration budget** of the model. For $C = 0$ we have a maximal margin classifier, since no errors are tolerated. Note that if C = 1 there could be no misclassifications, but some values could fall inside the margin.\n",
    "\n",
    "**The larger the C, the larger the number of support vectors and the lower the variance (but possibly the higher the bias!)**\n",
    "\n",
    "The soft margin classifier **can always be learned**, as opposed to the MMC. They are both robust with respect to **trivial observation**, aka observations that are not support vectors. This is not the case for trees, since they are based on the number of errors in observations instead of using only pivotal ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "The equation for the decision boundary can be generalized as\n",
    "\n",
    "$$f(x^*) = \\beta_0 + \\Sigma^n_{i = 1}\\alpha_iK(x^*, x_i)$$\n",
    "\n",
    "Where $K(x^*, x_i)$ is a function called **kernel** that, given two p-sized arrays, return a value. The intuition for kernel is to **map nonlinear data from a space of p dimension to a space of $p'$ dimensions (with $p' > p$) in which data can be divided with a linear decision boundary**. The axis of this new space represent the distance between the observation and the relative support vectors. Since an hyperplane in this space represent a distance, on the base plane this corresponds to a **round-like shape around the support vector of a specific class**. $\\alpha_i$ defines the hyperplane in R p'.\n",
    "\n",
    "Different types of kernel exists: the most commonly used is the **radial kernel** (also called RBF or Gaussian Kernel). Graphically, it is a round shape which tries to isolate class values around different support vector. The **polynomial kernel** is a d-degree line which passes around support vectors.\n",
    "\n",
    "To use SVM on multiclass classification problems, we have two possible approaches:\n",
    "\n",
    "* **one-vs-one**: We learn a binary classifier for each pair of classes ($\\frac{K (K - 1)}{2}$ for K classes), and then we choose the most frequently predicted class.\n",
    "\n",
    "* **one-vs-all**: We learn a binary classifier for each class against all the others (y = 1 for the chosen class, y = -1 for all the others). We estimate the probability for each classifier and choose the classifier with highest value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
